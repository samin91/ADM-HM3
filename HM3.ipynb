{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we open the file and read it line by line. Using for loop to clean data set( romoving \" and \\t).\n",
    "create list of column for each line and clean it.\n",
    "create records and join cols, separate them with tab.(Note that in a .tsv file, each column is separated by the tab.)\n",
    "create tsv file and write corresponded records(Store the documents in a directory with inside one file per house review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"Airbnb_Texas_Rentals.csv\",encoding=\"utf8\")\n",
    "lines = file.readlines()[1:]\n",
    "for line in lines:\n",
    "    cols = line.replace('\"', '').replace('\\t', ' ').split(',')\n",
    "    record = \"\\t\".join(cols[1:])\n",
    "    with open('doc\\doc_{}.tsv'.format(cols[0]), 'w', encoding=\"utf8\") as file:\n",
    "            file.write(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need all path of files(docpaths) that created in previous section to open and read them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "docpaths = [f for f in listdir(\"doc\") if isfile(join(\"doc\", f))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we create two function, one for saving the dictionary, and another for load it.\n",
    "Hint: Since you do not want to compute the inverted index every time you use the Search Engine, it is worth to think to store it in a separate file and load it in memory when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict_to_file(dic, file):\n",
    "    with open('{}.txt'.format(file), 'w', encoding=\"utf-8\") as file:\n",
    "        file.write(str(dic))\n",
    "    \n",
    "def load_dict_from_file(file):\n",
    "    with open('{}.txt'.format(file), 'r', encoding=\"utf-8\") as file:\n",
    "        data = file.read()\n",
    "        return eval(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first common step, you must preprocess the documents by\n",
    "\n",
    "   Removing stopwords\n",
    "   Removing punctuation\n",
    "   Stemming\n",
    "   Anything else you think it's needed\n",
    "\n",
    "first we tokenize the documents and for each token repeat this process:\n",
    "stemmer is for stemming the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['now', 'want', 'cre', 'two', 'diff', 'search', 'engin', 'giv', 'input', 'query', 'return', 'hous', 'match', 'query']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "#import inflect\n",
    "\n",
    "#in_engine = inflect.engine()\n",
    "stemmer = nltk.stem.LancasterStemmer()\n",
    "def clean_document(document):\n",
    "    tokens = nltk.word_tokenize(document)\n",
    "    filtered_words = []\n",
    "    for word in tokens:\n",
    "        if word not in nltk.corpus.stopwords.words('english'):#Removing stopwords (list of stop words )\n",
    "            word = word.lower()\n",
    "            #if word.isdigit():\n",
    "            #        word = in_engine.number_to_words(word)\n",
    "            word = re.sub(r'[^\\w\\s]', '', word)   #Removing punctuation\n",
    "            if(len(word) > 0):\n",
    "                word = stemmer.stem(word)\n",
    "                filtered_words.append(word)\n",
    "            \n",
    "    return filtered_words\n",
    "#print(clean_document('Now, we want to create two different Search Engines that, given as input a query, return the houses that match the query.'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create vocabulary and inverted index dictionary \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "vocabulary = {}\n",
    "inverted_index = {}\n",
    "indexOfWord = 0\n",
    "for doc in docpaths:\n",
    "    with open('doc\\{}'.format(doc), 'r',encoding=\"utf8\") as file:\n",
    "        lines = file.readlines()\n",
    "        if len(lines) > 0 :\n",
    "            document = lines[0]\n",
    "            cols = document.replace('\\\\n',' ').split('\\t')[:-1]\n",
    "            document = '\\t'.join(cols)\n",
    "            filtered_words = clean_document(document)\n",
    "            for w in filtered_words:\n",
    "                if w not in vocabulary:\n",
    "                    indexOfWord += 1\n",
    "                    vocabulary[w] = indexOfWord\n",
    "                if vocabulary[w] not in inverted_index:\n",
    "                    temp = []\n",
    "                    temp.append(doc)\n",
    "                    inverted_index[vocabulary[w]] = temp\n",
    "                elif doc not in inverted_index[vocabulary[w]]:\n",
    "                    inverted_index[vocabulary[w]].append(doc)\n",
    "save_dict_to_file(inverted_index,\"inverted_index\")\n",
    "save_dict_to_file(vocabulary,\"vocabulary\")\n",
    "print('finish')        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = load_dict_from_file('inverted_index')\n",
    "vocabulary = load_dict_from_file('vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input()\n",
    "query_tokens = clean_document(query)\n",
    "\n",
    "result_inverted = {}\n",
    "for token in query_tokens:\n",
    "    if token in vocabulary:\n",
    "        word_index = vocabulary[token]\n",
    "        result_inverted[token] = inverted_index[word_index]\n",
    "result_set = set()\n",
    "for item in result_inverted:\n",
    "    result_set.update(set(result_inverted[item]))\n",
    "\n",
    "final_set = set(result_set)\n",
    "for item in result_inverted:\n",
    "    final_set.intersection_update(set(result_inverted[item]))\n",
    "print(final_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def tf(word, dcoument):\n",
    "    return dcoument.count(word) / len(dcoument)\n",
    "\n",
    "def word_containing(word, documentlist):\n",
    "    return sum(1 for dcoument in documentlist if word in dcoument)\n",
    "\n",
    "def idf(word, documentlist):\n",
    "    return math.log(len(documentlist) / (1 + word_containing(word, documentlist)))\n",
    "\n",
    "def tfidf(word, document, documentlist):\n",
    "    return tf(word, document) * idf(word, documentlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "docpaths = [f for f in listdir(\"doc\") if isfile(join(\"doc\", f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentlist = {}\n",
    "indexOfWord = 0\n",
    "for doc in docpaths:\n",
    "    with open('doc\\{}'.format(doc), 'r',encoding=\"utf8\") as file:\n",
    "        lines = file.readlines()\n",
    "        if len(lines) > 0 :\n",
    "            document = lines[0]\n",
    "            cols = document.replace('\\\\n',' ').split('\\t')[:-1]\n",
    "            document = '\\t'.join(cols)\n",
    "            filtered_words = clean_document(document)\n",
    "            documentlist[doc] = filtered_words\n",
    "\n",
    "save_dict_to_file(documentlist,\"documentlist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentlist = load_dict_from_file(\"documentlist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = {}\n",
    "for key,doc in documentlist.items():\n",
    "    for w in doc:\n",
    "        score = tfidf(w,doc,documentlist)\n",
    "        w_index = (key,score)\n",
    "        if vocabulary[w] not in inverted_index:\n",
    "            temp = []\n",
    "            temp.append(w_index)\n",
    "            inverted_index[vocabulary[w]] = temp\n",
    "        elif doc not in inverted_index[vocabulary[w]]:\n",
    "            inverted_index[vocabulary[w]].append(w_index)\n",
    "\n",
    "save_dict_to_file(inverted_index,\"inverted_index_tfidf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
